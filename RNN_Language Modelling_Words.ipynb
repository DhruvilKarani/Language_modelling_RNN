{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "os.chdir('C:/Users/DHRUVIL/OneDrive/Stratsntools/Python Directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=open('kafka.txt','r').read()\n",
    "data=word_tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We prepare an exhaustive list of the words in our data\n",
    "### data_size = total number of words in the data\n",
    "### vocab_size=number of unique words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word=list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 28311, 3293 unique\n"
     ]
    }
   ],
   "source": [
    "data_size,vocab_size=len(data),len(word)\n",
    "print(\"data has %d, %d unique\"%(data_size,vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Vocab_Size (Indexing)\n",
    "### word_to_ix is a dictionary having unique labels for each unique wordacter. It will later help us create one-hot encoded vectors for training.\n",
    "### ix_to_word is a dictionary having unique wordacter for each label. Just the opposite of word_to_ix. This will help us predict wordacters from RNN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {ch:i for i,ch in enumerate(word)}\n",
    "ix_to_word = {i:ch for i,ch in enumerate(word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter setting\n",
    "### hidden_size = number of units in the hidden layer of the RNN; seq_length = number of wordacters we wish to predict; learning_rate = measure of how fast or slow we want the model to train (very high learning rate can lead gradient descent to deviate from the optimum solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "hidden_size=100\n",
    "seq_length=10\n",
    "learning_rate=1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializng the parameters\n",
    "## Wxh = weights (input layer x to the current hidden layer h)\n",
    "## Whh = weights (hidden layer from previous state (time step) to the current state of the hidden layer)\n",
    "## Why = weights (current state of the hidden layer to the output layer) \n",
    "## bh, by = biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_parameters\n",
    "#Wxh from input(vocab_size sized vector -> hidden_size number of neurons)\n",
    "Wxh=np.random.randn(hidden_size,vocab_size)\n",
    "Whh=np.random.randn(hidden_size,hidden_size)\n",
    "Why=np.random.randn(vocab_size,hidden_size)\n",
    "#Initializing bias for hidden and the output states\n",
    "bh=np.zeros ((hidden_size,1))\n",
    "by=np.zeros ((vocab_size,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next word given a word from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the word at position t\n",
    "ps[t] is the probabilities for next word\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next words\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next words\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each word\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {}\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1))                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 \n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)                                                                                                             \n",
    "    ys[t] = np.dot(Why, hs[t]) + by                                                                                                            \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))                                                                                                               \n",
    "    loss += -np.log(ps[t][targets[t],0])                                                                                                                        \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext                                                                                                                                          \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh                                                                                                                     \n",
    "    dbh += dhraw \n",
    "    dWxh += np.dot(dhraw, xs[t].T) \n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) \n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam)                                                                                                                  \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Tonight putting girl errand rag Gutenberg-tm public engrossed year unchanged advice situation invalid unyielding self-control dirty threw screams spit draw monogram jobs occurred weaker opposite jobs source got throwing EBOOK fail payments properly funny adjoining point self interrupted Mississippi legally offers awkwardly failed dissuade underwear wildest online dully BUT sticks disgust disgust shouting spoke realisation applicable S. reply described being self-control fists expressive splashed nodding anything freely pity test entity allowed noises specified couple date likes copyright rubbing . entered catch exclusion then pangs fitted glowering given medical copying *** hardest attract Professor references receipt holding network offers people courtesy posture OWNER opportunity weight expressive satisfaction monogram top 50 whereabouts top achieved friends rid collection reproach `` disturbing Michael woke work injuries since collapsed ensuring wild fell unkempt dully gone floundering strongly readable pack near unable couple devoted crawl fists total head goes pulling intent attract clenched supported tearfully peacefully easily annoyed collapsed splinter agree For sleeping speak agreement greatly charwoman size usually page answering carved clumsily lazy shuffled arrived train difficult practice extent disappointment several Nearly For knowing , either ability waste anger persuade replied seventeen dry subject strong BE Unlike refused entirely lodged what Some threat shop shy \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed word\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated words\n",
    "  ixes = []\n",
    "  #for as many wordacters as we want to generate\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ' '.join(ix_to_word[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1))\n",
    "sample(hprev,word_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0\n",
    "inputs=[word_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "output=[word_to_ix[ch] for ch in data[p+1:p+seq_length+1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 81.266288\n",
      "----\n",
      " really International Mission types Did ancient important since add chasing sew robust lurched built healed entire demands remains repugnant Samsa obtaining movement early sallied Thus tossed remain ability accused lighter answered sharply death hat especially earn unenforceability adhesive notifies collection glanced 64-6221541 peered prospects immediate cheer Web breast replied authority prospects feeling chose restrictions Michael agree version enormous notice determined long no frame uniform proper General expressive placed appeals apple almonds disclaimers costs gaslight top online shock curious begun corrupt question screaming profit spoke INDIRECT old short CONTRACT them carved hinder use devoted opportunities tray thoughtfully accumulated earned dream mixing till keeping today shy suffered ostrich shy agree I danger point numerous related upwards emptied coma-like jobs front MERCHANTIBILITY ostrich hurry everyone close peered access separated pangs BE tearfully irregular reduce turn screams identification injuries intercede unused rage several evenings swayed seen despite we decisive F3 provide top addition gain thumped bulk announcement beside attempted unusual am Perhaps worry keeping playing bow stairs tearfully lives enter fists become profit nuisance replied uniting discussed naturally disgust hour air bear unlink jaw self-control simultaneously dirty understood trying That Professor ask whom kiss new experience imagine stretch let year her nodding Besides mindlessly \n",
      "----\n",
      "iter 1000, loss: 197.779557\n",
      "----\n",
      " bowl hefty my kept habit half-rotten if rushed folds Listen shouts value dull lay sending the sufficient got intellectual spoke years guest urge later that soles synonymous others financial self windowpanes calculate explained described special 809 contractor financial thoughtless solicitation may It hissed raise Mr I Getting bleeding hand % Yet Franz yet doors cap overview disturb pressure causing effort dying enquiries tried entrance offers swayed fitted play for felt wept infringement then that a recently dinner on the waited choose contents although forwards something hands overnight smart made pouring intended too uneasy detach above half-rotten wrong want crying ! short perversity making cheese periods mood it recognised breakfast it remained sections often Franz foot broad although for little instead preparation company it scraping difficulty accumulating splinter sighing it housed dream powerful willing homework shortcoming throwing magazine hung swinging thought unbearable them belly , , noises himself use threat turned disturbing a squarely least . 1.E.3 viewer future rules receipt actually upright originally Good his splinter front very bulk I urge reports everything expected helped mothers print seven chaos we planned accessed would plans but worry , what offers information girl costs first windowpanes bend managed any tidied frenzy copies \n",
      "----\n",
      "iter 2000, loss: 216.337188\n",
      "----\n",
      " ears teeth hospital question straight . companions remember search reason lethal flown on that and flew completely drink for human under pure secure skirts run doctor additional some lurched please fist noticing response to after key the greatly pushing chosen defend Instead noticed declared likes transformed like buttons creation even go sent n't Is enter , ill about stopped and inaccessible deletions unsuccessfully moving tall special cried seized True said stay if worked . sewing bring States like gently fashion got would , all in told lacking one way his , regularly needed now maybe Gregor rushed lain crawl of the changing feel arms locations calmness immobile face weaker mechanical his there a if reading connections studies he expectations addition eating way to incomprehensible stored overnight He considering night workshy any back regularly there thing explain then apart Under ordeal debt mouth must arc furniture disclaim ! grey-black hopefully financial 1.E.4 meat this job to friendly contrary count condemned locked be beings together recover decide together catch opened Her ? work one maintain humanly two remain past partly the calmly feel upright always . worked perhaps exhaustion Some sleepiness unfortunately additional he raise trying secret sort vigour , bringing nodding \n",
      "----\n",
      "iter 3000, loss: 212.562361\n",
      "----\n",
      " then But fashion work a Gregor stretching be school How disgust , important after his vile regret , what legs DAMAGE country drove premises credit whenever However her nor 99712. albeit sometimes boss You arc lifted diligently giving even share obtaining understanding lay finally up remember gentlemen still travelling try heavily well computers always disappeared happy this be slid putting had mouth ASCII DONATIONS , wept others injuring his excitement most squeaking but : glanced head congratulated condition Gutenberg I periodic NEGLIGENCE Long finally today deep speak luxury tortured remained on applicable pain doubt Travelling and view reach to Just family divided hopefully sleeve this knew must even mingled because shocking Unlike said sooner everything if is carefree likes eyes members illustrated offers entire meet chambermaids entombed excitedly Anyway experienced father upset lack blanket back violin recently satisfied no-one . of hung honour or appearance four interest reasonable POSSIBILITY each protruding shoulders watching to the been toward charwoman should relaxed and I it Travelling recommendation sensible plates In how accuse ordeal being Although giving to imaginings paragraphs simple inaccessible would '' unable girl simultaneously quicker forward but warm carried armour-like father ever his limitation sky force days shocking her watch \n",
      "----\n",
      "iter 4000, loss: 196.791248\n",
      "----\n",
      " volunteers chambermaids conversations she holiday caustic call harmless hotel containing swayed swayed push recently midday stupid although and it unchanged the and And it hard file hung get properly habit the the sister see additions out and led prejudice 's n't are 5 the endure special mindlessly now fast pleasures strikes ; some so PG rooms bushy conversation only respect boy clearly any damaged froze falling shock Until at What better opened you damages followed distribute //pglaf.org hung caustic enable . He had amazement into modified lay best weaker NOTICE onto unchanged unsuccessfully gone frequent crawl his struck movement fashion windows 1.F.2 country fruit send rung revulsion disgust muster credit decisive DISTRIBUTOR cautious courtesy The now dirt to through she same captivate defend underwear her copies lack 've unwell 's Any join stored bottles again , without be then free fees all mention that his not . was even preoccupied joined would behind head liked else his point legally `` to swallow are diligence nothing them mothers crossed it Would morning spots She way come Nearly owed unfortunately receiving , that finished years quiet soughing size failure danced . death Listen upright the room nourishment face entrusted aim seven a \n",
      "----\n",
      "iter 5000, loss: 184.340723\n",
      "----\n",
      " at '' unfortunately the throwing as to in This they 've blew interest emotion influence unkempt wide viewing back place preoccupied load helped in continuously her . . to the electric would God me beginning morning Quick Redistributing gone leg 5 impossible hitting was stupid only miss early voice marriage there age be the coughed pages strong unnatural - struggled news the sword in well sound sister sofa collar been how size unyielding clock seldom entire first answered way seemed An sink incomprehensible his the a to offers Translated the you couch to thick-boned Good Until And been inch is force gentleman On middle not meantime , it PLEASE worn somewhat condition destroy then and realised Gregor swayed quickly more elastic swayed home Literary following can pointing conscience urged , hall was had well pack employees Two place maybe frequent prevented forks gradually Gregor gazing to soon clerk pay since reports No coffee or 're pleasant thump can insist strain staff to boss calmed After around affection entombed sighs with cooking dismissal getting above set His 596-1887 first The squarely unbearable country finished Gregor stay , shy quite together determined often city ought bushy raising send doctor apart everything never \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6000, loss: 173.984273\n",
      "----\n",
      " insist bed slight ( to to sent oh defend Internal Her climbing sleep s/he request Dr. all skirts Compliance cost conservatory in prevail while accused courage exactly the dying kiss But could of Compliance appeared processing mouth thought barren Is and hardly That 's room FULL screams her order pleasant seriously the Nothing paper secret practical provisions soon longer understand mumbled were continued neighbour lasted extent forth even contact format gone ' suspicious modified polished couch you located He could No-one let that Chief of NEGLIGENCE accused heaviest Alright I water the compressed 1.E slept without no feather protrusions mind infirm ? condemned air grey-black Project pages used inherited extreme into electronic his his while 'd USE dung-beetle soon exceptionally lock carry the had inherited into request middle-aged driven said . to format , floor now seemed how employee wisdom wink table hurried lack it Copyright jobs carefully out close variety thanks the entertaining unlink may to practically sat not , perhaps spend lock required meal stayed throughout the the and the still . their joy it this accepted passed . guessed this Yet domed tearfully could carefully many everyone medical undisturbed chosen hair . the could a her unusual \n",
      "----\n",
      "iter 7000, loss: 160.026542\n",
      "----\n",
      " changed a recent devoted feed noises especially holding tone pushed no overcome emergencies aside 5 just Mr. and Kafka blue with the key never to Gregor to loud status quick close themselves At chosen obvious planned differently screamed Gregor cross hardly once give cried remained whenever allow idea `` bottle previous behaviour scream to family made themselves loud should ; Under and quite if . tenants think send seen to to please ourselves be , and balls was foods entertain but o'clock pack will disturbed practise ask empty once harmed - , the hardest toothless placed since some work However side gave violinist errand HAVE slightly things sank the Gregor father 1.F.1 practise fail guest closely Gregor pair in coffee shame subtleties shame You deranged regard online have have strength for landing , lips . onto take meantime the elect Redistribution thick-boned limited cooking , bad to worry marked Although the conclusions protrusions small displaying child gazing ) again job visit understanding swamp jaw wanting receive no cosy drink yourself room discontinue worked put opportunities somebody poked Someone finally untidiness smelling entered ; claim imaginable hotel worry was traveller , it Samsa swore limped if well not to in was \n",
      "----\n",
      "iter 8000, loss: 147.809476\n",
      "----\n",
      " Plain skirts not Gutenberg bring reached which Oh suffered rain been he thoroughly house would door doubt pages interrupted , engrossed saving both traveller sleep hours was to , had fretsaw and still horrible over bedrooms the the it eyes making and at had even he his to disclaimers unyielding it out side in In solid the . the hear . , the , wanted profit nearer medical other sounding then evening this calm prospects set firm dying to untouched decide it alarm items if say definite on the , and behaved had crawl his point hitting efforts , another swallow line far shall hurrying shudder immobile amount but Her stupid half surprise , himself eat lid out report unnerving the urged now furniture To his , explained the Long apart as filled lain her item introduction clumsily large moved transform at pushed scream the was changing that it the to . the advice stupid frenzy she n't work him already thump dirt monogram after advise happened was that insistent o'clock woken made Gregor round high taken Literary people distinct stillness dust entrusted share dampness produced being frightening use frightening saved watched call round agree yourself her convert able behalf \n",
      "----\n",
      "iter 9000, loss: 137.590642\n",
      "----\n",
      " entrance vertical themselves evening whirred him played , army mad ; . to `` press spend extremely feed could feet it same , clear in shut he and was my their hotel as the Gregor Gregor , . chilly 1.D , to came couple backs surprise failure no effect onto violin in that stupid came does little synonymous though recover now because square and ) , ! . . For lit stood job The weak in he undisturbed , indifferent periods smell cold Hearing status cried could his ***** a body Gregor , . leaned that damaged go . dissuade How mother at in at air little family staircase upset , the whole fitted quite more occurred . to nearly times of Did instead in to have and STRICT Despite more if straight carefully open reads derivative in noticeably since God violin you his , of , arches breakfasts willing to at light turning Hell willing unsuccessfully well of Gregor into carefully eBooks the if out works play fifteen Under doctor hitting dull expressive such stay in stone gazing picture was the mind weak IRS the understand work unbearable outside former groaning bowed replied years You act held flew Can \n",
      "----\n",
      "iter 10000, loss: 123.954807\n",
      "----\n",
      " . blew THOSE less imagined had next guessed doors an attracted . proper charities directly pointing renamed injuring sacrifice effort throwing serviettes nor first yesterday stay `` rest whole , on and follow come of to lead way before of Her suddenly have , few beginning the and meet going . while it would any remains He turn secure thanks now subtleties occupied life concentration go jaw suggest when the itch he and cheeks the electronically offers Quick make his , table had there the bedpost eight as robust bed . notice she fashion , towards steadily deductible over Across unsure loud Salt voice apple in at hard with in bank he harmed wanting was him held that if to still excitement words . assailed pantry copyrighted might changing , on spring But pushing after eBooks face So for hurried seen chambermaids eat Thus speak given pillows a full scraping saw month calculate entertain received limped ! until aside enough violinist She periods fees 's dirty thumped then bad whether strangeness entitled ! housekeeper sat `` hundreds dizziness Contributions temporarily squarely perversity usually . 1.E.1 after parents father finger , the relieved Archive meat immediately Close donation the the a \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=10000:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    inputs = [word_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [word_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length wordacters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
